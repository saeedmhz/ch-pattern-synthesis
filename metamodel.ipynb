{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aece796-5ac7-4c89-bca1-698a88b14c78",
   "metadata": {},
   "source": [
    "# Mechanical MNIST Cahn-Hilliard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e1d260-cd4c-489c-9375-5abcbcacacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models, activations, callbacks#metrics, callbacks, regularizers\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea950c-ba40-44a2-9174-00a300aca909",
   "metadata": {},
   "source": [
    "## Metamodel\n",
    "**Network Structure:** A Convolutional Neural Network (with 9 hidden layers) is used as a metamodel for single quantity of interest prediction (strain energy).  \n",
    "**Input:** 64x64 images of material distribution (cahn-hilliard patterns)  \n",
    "**Output:** strain energy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcd3163-e699-488a-9c37-0fcea168f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network structure in tensorflow\n",
    "def create_model_cahnhill():\n",
    "    # create a simple CNN model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation=None, input_shape=(64, 64, 1)))\n",
    "    model.add(layers.BatchNormalization(axis=1))\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation=None)) #input: 32x32 with 32 channels\n",
    "    model.add(layers.BatchNormalization(axis=1))\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation=None)) #input: 16x16 with 64 channels\n",
    "    model.add(layers.BatchNormalization(axis=1))\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='valid', activation=None)) #input: 8x8 with 128 channels\n",
    "    model.add(layers.BatchNormalization(axis=1))\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='valid', activation=None)) #input: 6x6 with 256 channels\n",
    "    model.add(layers.BatchNormalization(axis=1))\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "    #model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(512, (4, 4), padding='valid', activation=None)) #input: 4x4 with 256 channels\n",
    "    model.add(layers.BatchNormalization(axis=1))\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "    model.add(layers.Conv2D(512, (1, 1), padding='valid', activation=None))\n",
    "    model.add(layers.BatchNormalization(axis=1))\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "    model.add(layers.Conv2D(128, (1, 1), padding='valid', activation=None))\n",
    "    model.add(layers.BatchNormalization(axis=1))\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "    model.add(layers.Conv2D(1, (1, 1), padding='valid', activation=None))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844d446-36c4-489d-841d-9fd674cc997d",
   "metadata": {},
   "source": [
    "To see a model summary run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea5f55-ba6e-4617-bf54-fa9f1f61cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_cahnhill()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df1900-d558-4c99-bd8b-0129a283fded",
   "metadata": {},
   "source": [
    "## Datasets and Training Procedure\n",
    "**Core Dataset:** A fixed subset of the real Cahn Hilliard patterns with 1000 samples  \n",
    "**Generated Patterns:** 59000 patterns generated using the StyleGAN network trained on the **Core** dataset  \n",
    "**Extra Real Patterns:** 59000 patterns to extend the size of training set and 12000 patterns for the test set  \n",
    "\n",
    "We use a **5-fold cross-vallidation** to evaluate the metamodel performance.\n",
    "\n",
    "**Method:**\n",
    "The core dataset is partitioned into 5 subsamples each containing 200 samples. One subsample is choosen as the validation set to choose the best model during the training process and the other 4 subsamples are used as training data and the process will be repeated 5 times with different validation sets.  \n",
    "The training set size can be extended with **Real** or **Generated** patterns. We add $k = \\{0, 1000, 3000, 7000, 15000, 31000, 59000\\}$ extra patterns to the training set. In addition, rotation $\\{90^\\circ, 180^\\circ, 270^\\circ\\}$ can be used to augment the training set.\n",
    "\n",
    "**Note**  \n",
    "Strain energy values for low fidelity simulation are in the order of $10^{-6}$ since only perturbation analysis is considered. This causes some problem for the convergence of network parameters. Therefore, we multiply output values by $10^6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ed9da5-7b90-4688-af22-cd23e9666575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "core_dataset = [np.load('lowfi-inputs.npy').reshape(-1,64,64,1)[0:1000], 1e6*np.load('lowfi-strain-energy.npy')[0:1000]]\n",
    "extra_real = [np.load('lowfi-inputs.npy').reshape(-1,64,64,1)[1000:60000], 1e6*np.load('lowfi-strain-energy.npy')[1000:60000]]\n",
    "extra_generated = [np.load('generated-scc.npy').reshape(-1,64,64,1), 1e6*np.load('generated-scc-energy.npy')]\n",
    "test_dataset = [np.load('lowfi-inputs.npy').reshape(-1,64,64,1)[60000:], 1e6*np.load('lowfi-strain-energy.npy')[60000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec97a2e7-0a60-46b9-8d82-478b1cade463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(core_data, extra_data, k=0, rot_aug_flag=False, extra_data_type=''):\n",
    "    for fold in range(5):\n",
    "        res_folder = 'results'\n",
    "        ########## partitioning the core dataset ##########\n",
    "        # inputs\n",
    "        a1, a2, a3 = np.split(core_data[0], [fold*200, (fold+1)*200])\n",
    "        train_images = np.vstack((a1, a3))\n",
    "        validation_images = a2\n",
    "\n",
    "        # labels\n",
    "        a1, a2, a3 = np.split(core_data[1], [fold*200, (fold+1)*200])\n",
    "        train_labels = np.hstack((a1, a3))\n",
    "        validation_labels = a2\n",
    "\n",
    "        ########## adding extra samples to the training set ##########\n",
    "        if k:\n",
    "            perm = np.random.permutation(800+k)\n",
    "            train_images = np.vstack((train_images, extra_data[0][0:k]))[perm]\n",
    "            train_labels = np.hstack((train_labels, extra_data[1][0:k]))[perm]\n",
    "            res_folder = res_folder+f'-{extra_data_type}-{k}'\n",
    "\n",
    "        ########## training set augmentation with rotation ##########\n",
    "        if rot_aug_flag:\n",
    "            train_images = np.vstack((train_images, np.rot90(train_images, 1, [1,2]), np.rot90(train_images, 2, [1,2]), np.rot90(train_images, 3, [1,2])))\n",
    "            train_labels = np.hstack((train_labels, train_labels, train_labels, train_labels))\n",
    "            res_folder = res_folder+f'-wAug'\n",
    "\n",
    "        ########## Training Process ##########\n",
    "        Path(res_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        optim = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "\n",
    "        # save best model\n",
    "        mcp_save = callbacks.ModelCheckpoint(filepath=res_folder+f'/model-'+str(fold)+'.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "        \n",
    "        # save training and validation error at each epoch\n",
    "        csv_logger = callbacks.CSVLogger(res_folder+f'/results-{fold}.csv')\n",
    "        \n",
    "        my_callbacks = [csv_logger, mcp_save]\n",
    "\n",
    "        model = create_model_cahnhill()\n",
    "        model.compile(optimizer=optim,loss='mse')\n",
    "\n",
    "        history = model.fit(train_images, train_labels, epochs=200, batch_size=64, callbacks=my_callbacks, validation_data=(validation_images, validation_labels))\n",
    "        \n",
    "def evaluate(data, model_name):\n",
    "    model = create_model_cahnhill()\n",
    "    model.compile(loss='mse')\n",
    "    \n",
    "    print(model_name)\n",
    "    model.load_weights(model_name)\n",
    "    mse = model.evaluate(data[0], data[1], batch_size=64)\n",
    "    pred = model.predict(data[0])\n",
    "    coeffs = np.polyfit(pred.reshape(-1), data[1].reshape(-1), 1)\n",
    "    p = np.poly1d(coeffs)\n",
    "    pp = p(pred)\n",
    "    r2 = r2_score(pp, data[1])\n",
    "    \n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87f0ba-8a7d-4b61-ab7c-b04e56047368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a model on the core dataset with 1000 extra generated samples without rotational data augmentation\n",
    "training(core_dataset, extra_generated, k=1000, rot_aug_flag=False, extra_data_type='generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2a10c-2f0a-497c-9a1b-4a03633ccbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model performance on the test set\n",
    "mse = np.zeros(5)\n",
    "r2 = np.zeros(5)\n",
    "for fold in range(5):\n",
    "    mse[fold], r2[fold] = evaluate(test_dataset, f'results-generated-1000/model-{fold}.h5')\n",
    "\n",
    "print('\\nAverage R2 Score of the trained models:', np.mean(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1b321-44b2-4ecf-bb93-a187bc5c733a",
   "metadata": {},
   "source": [
    "# Cahn-Hilliard Pattern Synthesis\n",
    "We used [StyleGAN2 with Adaptive Discriminator Augmentation](https://github.com/NVlabs/stylegan2-ada-pytorch) for patterns synthesis with a generative model. To install all requirements and prerequisites of for library please follow the instruction in [stylegan2-ada-reqs](https://github.com/NVlabs/stylegan2-ada-pytorch#requirements).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e5ac8-1994-4722-b7bc-ad033df414c0",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "We use 1000 samples in the core dataset to train a generative model. To prepare the dataset for training, we need a folder containing images of Cahn-Hilliard patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82655165-93f5-4262-9527-266126a13eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
